
## 2.2.1 Measuring the Quality of Fit

Mean Squared Error : 
$$ MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i -  \hat{f}(x_i))^2 $$

Is the most commonly used measure to quantify the extent in which the predicted response is close to the true value.

- We only care about prediction of the models for future setting (unseen data). 
- We want to choose a method that gives the lowest $MSE$ (or any metric) on the test data (unseen) data as opposed to the lowest training $MSE$. 

**Overfitting**: Low training $MSE$, and high test $MSE$. 

### 2.2.2 The Bias-Variance Trade Off

The expected test $MSE$ at $x_0$ :
$$ E (y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon) $$
- $Var(\epsilon)$ is the irreducible error, i.e. the least amount of error achievable. 
- A good statistical learning method would reduce the variance and bias terms, but its hardly the case. 
![[Pasted image 20251022093306.png]]

**Variance**: The amount by which $\hat{f}$ would change if we estimate it using a different training set. i.e. sensitive to change.

**Bias**: Error introduced by a model's inability to capture the true underlying pattern in the data.

- Flexible models $\uparrow$ variance $\downarrow$ bias.

---
### 2.2.3 The Classification Setting 

- The above point were made on a regression models, but could be transfer to a classification models as well. 

**Training Error Rate**: The proportion of mistakes that are made if we apply the estimate $\hat{f}$ to the training observation.

$$ \frac{1}{n} \sum_{i=1}^n I(y_i \neq \hat{y_i}) $$
- If $y_i \neq \hat{y_i}$ $I=1$ 
- If $y_i = \hat{y_i}$ $I = 0$ 
- Thus its compute the fraction of incorrect classification. 

#### Bayes Classifier

### K-Nearest Neighbors

